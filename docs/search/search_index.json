{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"odinson-rest \uf0c1 What is it? \uf0c1 The odinson-rest is a REST-based interface to Odinson How do I use it? \uf0c1 For instruction in usage, see the tutorial after installing. Developing \uf0c1 For instructions on developing the odinson-rest , navigate to the Development section.","title":"Home"},{"location":"#odinson-rest","text":"","title":"odinson-rest"},{"location":"#what-is-it","text":"The odinson-rest is a REST-based interface to Odinson","title":"What is it?"},{"location":"#how-do-i-use-it","text":"For instruction in usage, see the tutorial after installing.","title":"How do I use it?"},{"location":"#developing","text":"For instructions on developing the odinson-rest , navigate to the Development section.","title":"Developing"},{"location":"authors/","text":"Author \uf0c1 Gus Hahn-Powell","title":"Contributors"},{"location":"authors/#author","text":"Gus Hahn-Powell","title":"Author"},{"location":"install/","text":"Install \uf0c1 Requirements \uf0c1 sbt docker = 8G of RAM REST API \uf0c1 Releases \uf0c1 We publish releases in the form of docker images: ?? Build \uf0c1 The project can be built using either docker or sbt; however, the recommended method is to use docker. Docker \uf0c1 We construct our docker images using the sbt native-packager plugin: sbt dockerize For information on additional tasks (generating Dockerfiles, publishing images, etc.), see this section of the native-packager documentation . Building docker images for other architectures \uf0c1 sbt \";clean; docker:stage\" cd rest/target/docker/stage/ docker buildx build --platform=linux/amd64 -o type=docker -t \"lumai/cgiar-reader-rest-api:amd64\" . sbt (Scala) \uf0c1 The REST API server can be launched directly using SBT: sbt web","title":"Installation"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#requirements","text":"sbt docker = 8G of RAM","title":"Requirements"},{"location":"install/#rest-api","text":"","title":"REST API"},{"location":"install/#releases","text":"We publish releases in the form of docker images: ??","title":"Releases"},{"location":"install/#build","text":"The project can be built using either docker or sbt; however, the recommended method is to use docker.","title":"Build"},{"location":"install/#docker","text":"We construct our docker images using the sbt native-packager plugin: sbt dockerize For information on additional tasks (generating Dockerfiles, publishing images, etc.), see this section of the native-packager documentation .","title":"Docker"},{"location":"install/#building-docker-images-for-other-architectures","text":"sbt \";clean; docker:stage\" cd rest/target/docker/stage/ docker buildx build --platform=linux/amd64 -o type=docker -t \"lumai/cgiar-reader-rest-api:amd64\" .","title":"Building docker images for other architectures"},{"location":"install/#sbt-scala","text":"The REST API server can be launched directly using SBT: sbt web","title":"sbt (Scala)"},{"location":"tutorial/","text":"REST API \uf0c1 The reader can be used through the REST API. After building the docker image, launch a container using the following command: Navigate to localhost:9000/api to interactively explore the API through the OpenAPI 3.0 specification. Python library \uf0c1 We also provide a Python library as a simple way to build applications that interact with the Odinson REST API. You can either connect to an existing odinson-rest service or launch one using docker. Launching and interacting with a service using docker \uf0c1 from lum.odinson.doc import Document, Fields from lum.odinson.rest.docker import DockerBasedOdinsonAPI # create a local index data_dir = \"/local/path/to/my/data/dir\" engine = DockerBasedOdinsonAPI(local_path=data_dir) # load an Odinson document doc_file = \"path/to/odinson/document.json\" doc = Document.from_file(doc_file) # index the document engine.index(doc) # query the index for res in engine.search(odinson_query=\"[lemma=be]\"): for span in res.spans(): print(f\"{res.document_id} ({res.sentence_index}): {span}\") Common Workflows \uf0c1 ?? sbt \uf0c1 Several command aliases are defined in the build.sbt . These can be altered and added to at the developers discretion. - dockerfile : Command for generating and publishing a docker image ( target/docker/stage ). \uf0c1 dockerize : Command for generating and publishing a docker image. documentize : Generates scaladoc and copies documentation to the docs/ directory.","title":"Usage"},{"location":"tutorial/#rest-api","text":"The reader can be used through the REST API. After building the docker image, launch a container using the following command: Navigate to localhost:9000/api to interactively explore the API through the OpenAPI 3.0 specification.","title":"REST API"},{"location":"tutorial/#python-library","text":"We also provide a Python library as a simple way to build applications that interact with the Odinson REST API. You can either connect to an existing odinson-rest service or launch one using docker.","title":"Python library"},{"location":"tutorial/#launching-and-interacting-with-a-service-using-docker","text":"from lum.odinson.doc import Document, Fields from lum.odinson.rest.docker import DockerBasedOdinsonAPI # create a local index data_dir = \"/local/path/to/my/data/dir\" engine = DockerBasedOdinsonAPI(local_path=data_dir) # load an Odinson document doc_file = \"path/to/odinson/document.json\" doc = Document.from_file(doc_file) # index the document engine.index(doc) # query the index for res in engine.search(odinson_query=\"[lemma=be]\"): for span in res.spans(): print(f\"{res.document_id} ({res.sentence_index}): {span}\")","title":"Launching and interacting with a service using docker"},{"location":"tutorial/#common-workflows","text":"??","title":"Common Workflows"},{"location":"tutorial/#sbt","text":"Several command aliases are defined in the build.sbt . These can be altered and added to at the developers discretion.","title":"sbt"},{"location":"tutorial/#-dockerfile-command-for-generating-and-publishing-a-docker-image-targetdockerstage","text":"dockerize : Command for generating and publishing a docker image. documentize : Generates scaladoc and copies documentation to the docs/ directory.","title":"- dockerfile: Command for generating and publishing a docker image (target/docker/stage)."},{"location":"dev/annotations/","text":"Annotations \uf0c1 The rules defined in the cgiar-reader grammars are run over annotated text, which is generated by a Processor to produce an annotated Document instance that can be serialized to JSON. Example JSON output for the text \"How many TEUs of Frozen Meat are heading to Hamburg?\" can be found in this Github Gist . Processors \uf0c1 Currently there are three processors avalaible to the REST API: Custom , CluProcessor , and ProxiedProcessor . CustomProcessor \uf0c1 The Custom processor uses a mixture of components from the clulab/processors library with custom pre- and post-processing hooks. CluProcessor \uf0c1 CluProcessor comes directly from clulab/processors . ProxiedProcessor \uf0c1 The ProxiedProcessor performs annotation via REST calls and allowing the user to easily mix components written in different programming languages. The ProxiedProcessor is defined in reader/src/main/scala/ai/lum/mr/processors . One can use SpaCy for annotation via the clu-spacy library ). Lexicon NER \uf0c1 The Lexicon NER allows you to specify many strings that should be given a certain Named Entity label which can help downstream processes recognize entities. To add new strings (or edit existing ones), follow these steps. Create a new file or edit the strings in the TSVs under the reader/src/main/resources/ai/lum/kb/raw/ folder. The TSVs are of the format TAB . If you add a new file, update the cgiar.tsv record to reflect what NE label the strings beneath should be assigned. Update ai.lum.mr.ner.kbs in reference.conf with the path where the new KB will be. Run sbt \"reader/runMain ai.lum.mr.cgiar.ner.CustomKbGenerator\" to bundle the TSVs into a format the LexiconNER can use. Create rules that make reference to the entity field (e.g. using [entity=/Crop/] and assign an appropriate taxonomy label (being sure to update taxonomy.yml).","title":"Annotations"},{"location":"dev/annotations/#annotations","text":"The rules defined in the cgiar-reader grammars are run over annotated text, which is generated by a Processor to produce an annotated Document instance that can be serialized to JSON. Example JSON output for the text \"How many TEUs of Frozen Meat are heading to Hamburg?\" can be found in this Github Gist .","title":"Annotations"},{"location":"dev/annotations/#processors","text":"Currently there are three processors avalaible to the REST API: Custom , CluProcessor , and ProxiedProcessor .","title":"Processors"},{"location":"dev/annotations/#customprocessor","text":"The Custom processor uses a mixture of components from the clulab/processors library with custom pre- and post-processing hooks.","title":"CustomProcessor"},{"location":"dev/annotations/#cluprocessor","text":"CluProcessor comes directly from clulab/processors .","title":"CluProcessor"},{"location":"dev/annotations/#proxiedprocessor","text":"The ProxiedProcessor performs annotation via REST calls and allowing the user to easily mix components written in different programming languages. The ProxiedProcessor is defined in reader/src/main/scala/ai/lum/mr/processors . One can use SpaCy for annotation via the clu-spacy library ).","title":"ProxiedProcessor"},{"location":"dev/annotations/#lexicon-ner","text":"The Lexicon NER allows you to specify many strings that should be given a certain Named Entity label which can help downstream processes recognize entities. To add new strings (or edit existing ones), follow these steps. Create a new file or edit the strings in the TSVs under the reader/src/main/resources/ai/lum/kb/raw/ folder. The TSVs are of the format TAB . If you add a new file, update the cgiar.tsv record to reflect what NE label the strings beneath should be assigned. Update ai.lum.mr.ner.kbs in reference.conf with the path where the new KB will be. Run sbt \"reader/runMain ai.lum.mr.cgiar.ner.CustomKbGenerator\" to bundle the TSVs into a format the LexiconNER can use. Create rules that make reference to the entity field (e.g. using [entity=/Crop/] and assign an appropriate taxonomy label (being sure to update taxonomy.yml).","title":"Lexicon NER"},{"location":"dev/developing/","text":"Developing The Reader \uf0c1 In developing the cgiar-reader there are three pieces: rules (and corresponding tests), taxonomy , and actions . The REST API and its endpoints are already defined. For information on how annotations are generated for use in rules, or how to use alternate processors, see the Annotations section. Rules \uf0c1 Before developing a new rule or set of related rules, it is best to first define tests which describe the expected behavior of the rule(s). To develop tests follow the Testing section. Rules in the cgiar-reader are defined using Odin. For an in depth look at Odin and writing a grammar, please see the manual . The cgiar-reader has two grammars, entities and events . The grammars can be found under reader/grammars/cgiar . An identical set of grammars can be found under reader/src/main/resources/ai/lum/reader/grammars/cgiar , however, when developing the grammars the user should only modify the top level grammars. These will later be edited and copied to the src grammars via action. Rules can be developed with live reloading following the instructions in the Development/Install section. Writing Rules \uf0c1 Odin rules are written in yaml and run over annotated text (the Odin manual includes a gentle introduction to YAML syntax ). Annotated text is produced using an external NLP service (such as StanfordCoreNLP or SpaCY ), however, Penn Tags and Universal Dependencies are always included in the annotations. There are two types of rules, token and dependency . If the type is not specified it defaults to type=dependency . Token rules are defined over the set of tokens and their values, while dependency rules are defined over the set of dependencies. Token rule If you wanted to label all tokens which carry the NER tag \"LOCATION\" as \"Location,\" the following rule can be used. - name: ner-loc label: Location priority: 1 type: token pattern: | [entity=LOCATION]+ Note Rules can be given a priority which determines the order in which rules apply. For example, a rule with priority: 1 will only be run on the first iteration, whereas a rule with priority: \"2+\" will be run on all iterations following the first. For example, given the text \"How many F16 engines are heading to The United Kingdom?\" the sequence \"The United Kingdom\" would be labeled Location by this rule. Graph traversal rule If you wanted to capture a \"risk of\" event, you could start with a simple rule defining a traversal over a syntactic dependency graph such as the following: - name: risk-of label: RiskOf example: \"What is the risk of spoilage for frozen fish heading to Dubai on August 24th 2020?\" pattern: | trigger = [lemma=risk] of type:Entity = nmod_of This rule finds all words whose lemma is \"risk\" and if followed by \"of\" labels the sequence as a trigger of a RiskOf event, which is given a type defined by the dependency relation \"nmod_of.\" In the provided example \"spoilage\" would be labeled as the type of the RiskOf event. Note When the rules are run on text, a JSON file of labeled mentions is generated. Mentions are the matches found by the rules within the text, and the labels are included in a heiarchy defined in the Taxonomy . Taxonomy \uf0c1 The taxonomy is a set of heiarchical relationships between mention labels. Like the grammars, the taxonomy can be found in two places but only the top level taxonomy should be modified in development. A sample of the cgiar-reader taxonomy can be seen here: - Measurement: - Unit - NumericExpression: - Quantity Actions \uf0c1 Generally, when developing rules there should be no need to change the existing actions. However, if it is necessary, new actions or modifications to existing actions can be made in reader/src/main/scala/ai/lum/mr/cgiar/odin/CustomActions.scala . For more information about actions, see How it Works .","title":"Developing"},{"location":"dev/developing/#developing-the-reader","text":"In developing the cgiar-reader there are three pieces: rules (and corresponding tests), taxonomy , and actions . The REST API and its endpoints are already defined. For information on how annotations are generated for use in rules, or how to use alternate processors, see the Annotations section.","title":"Developing The Reader"},{"location":"dev/developing/#rules","text":"Before developing a new rule or set of related rules, it is best to first define tests which describe the expected behavior of the rule(s). To develop tests follow the Testing section. Rules in the cgiar-reader are defined using Odin. For an in depth look at Odin and writing a grammar, please see the manual . The cgiar-reader has two grammars, entities and events . The grammars can be found under reader/grammars/cgiar . An identical set of grammars can be found under reader/src/main/resources/ai/lum/reader/grammars/cgiar , however, when developing the grammars the user should only modify the top level grammars. These will later be edited and copied to the src grammars via action. Rules can be developed with live reloading following the instructions in the Development/Install section.","title":"Rules"},{"location":"dev/developing/#writing-rules","text":"Odin rules are written in yaml and run over annotated text (the Odin manual includes a gentle introduction to YAML syntax ). Annotated text is produced using an external NLP service (such as StanfordCoreNLP or SpaCY ), however, Penn Tags and Universal Dependencies are always included in the annotations. There are two types of rules, token and dependency . If the type is not specified it defaults to type=dependency . Token rules are defined over the set of tokens and their values, while dependency rules are defined over the set of dependencies. Token rule If you wanted to label all tokens which carry the NER tag \"LOCATION\" as \"Location,\" the following rule can be used. - name: ner-loc label: Location priority: 1 type: token pattern: | [entity=LOCATION]+ Note Rules can be given a priority which determines the order in which rules apply. For example, a rule with priority: 1 will only be run on the first iteration, whereas a rule with priority: \"2+\" will be run on all iterations following the first. For example, given the text \"How many F16 engines are heading to The United Kingdom?\" the sequence \"The United Kingdom\" would be labeled Location by this rule. Graph traversal rule If you wanted to capture a \"risk of\" event, you could start with a simple rule defining a traversal over a syntactic dependency graph such as the following: - name: risk-of label: RiskOf example: \"What is the risk of spoilage for frozen fish heading to Dubai on August 24th 2020?\" pattern: | trigger = [lemma=risk] of type:Entity = nmod_of This rule finds all words whose lemma is \"risk\" and if followed by \"of\" labels the sequence as a trigger of a RiskOf event, which is given a type defined by the dependency relation \"nmod_of.\" In the provided example \"spoilage\" would be labeled as the type of the RiskOf event. Note When the rules are run on text, a JSON file of labeled mentions is generated. Mentions are the matches found by the rules within the text, and the labels are included in a heiarchy defined in the Taxonomy .","title":"Writing Rules"},{"location":"dev/developing/#taxonomy","text":"The taxonomy is a set of heiarchical relationships between mention labels. Like the grammars, the taxonomy can be found in two places but only the top level taxonomy should be modified in development. A sample of the cgiar-reader taxonomy can be seen here: - Measurement: - Unit - NumericExpression: - Quantity","title":"Taxonomy"},{"location":"dev/developing/#actions","text":"Generally, when developing rules there should be no need to change the existing actions. However, if it is necessary, new actions or modifications to existing actions can be made in reader/src/main/scala/ai/lum/mr/cgiar/odin/CustomActions.scala . For more information about actions, see How it Works .","title":"Actions"},{"location":"dev/documentation/","text":"Documentation \uf0c1 You can view the latest documentation at the cgiar-reader Website . API Documentation \uf0c1 We use scaladoc to generate our API documentation. To generate API documentaion use the following command: sbt doc This will generate HTML pages documenting the API for each subproject: reader : reader/target/scala-2.12/api/index.html rest : rest/target/scala-2.12/api/index.html Note These files are copied to the docs/api directory when the command sbt documentize is used. General Documentation \uf0c1 We use mkdocs to generate our site documentation from markdown. Markdown source files are located under the docs directory. To develop the documentation with live updates use the following command: docker run --rm -it -v $PWD:/app \\ -p 8000:8000 \\ parsertongue/mkdocs:latest \\ mkdocs serve -a 0.0.0.0:8000 Open your browser to localhost:8000 .","title":"Documentation"},{"location":"dev/documentation/#documentation","text":"You can view the latest documentation at the cgiar-reader Website .","title":"Documentation"},{"location":"dev/documentation/#api-documentation","text":"We use scaladoc to generate our API documentation. To generate API documentaion use the following command: sbt doc This will generate HTML pages documenting the API for each subproject: reader : reader/target/scala-2.12/api/index.html rest : rest/target/scala-2.12/api/index.html Note These files are copied to the docs/api directory when the command sbt documentize is used.","title":"API Documentation"},{"location":"dev/documentation/#general-documentation","text":"We use mkdocs to generate our site documentation from markdown. Markdown source files are located under the docs directory. To develop the documentation with live updates use the following command: docker run --rm -it -v $PWD:/app \\ -p 8000:8000 \\ parsertongue/mkdocs:latest \\ mkdocs serve -a 0.0.0.0:8000 Open your browser to localhost:8000 .","title":"General Documentation"},{"location":"dev/faq/","text":"Development FAQs \uf0c1 When do I have to rebuild the Docker image? \uf0c1 When you have changed build.sbt or any scala files in reader.src . How do I rebuild the Docker image? \uf0c1 To rebuild the Docker image, simply run sbt dockerize in your terminal. Make sure you are in the correct directory, which should be where you cloned the cgiar-reader repo. There are two identical grammars in different directories, where do I add rules? \uf0c1 Always add/change rules in the yml files under reader.grammars.cgiar . The changes made to these files will be updated in the src files automatically. How do I change annotation pipelines? \uf0c1 The config file passed to the API upon startup should include a specification for the preffered processor. Currently only \"ProxiedProcessor\", \"CluProcessor\", and \"Custom\" are supported processor options. To specify a custom processor use \"ProxiedProcessor.\" What variables can I specify in the config file? \uf0c1 The user can specify RULES_PREFIX and PROCESSORS_SERVICE_URL . The latter is the url of the proxied processor service. What sbt command line options are there? \uf0c1 To see what commands are available, run sbt tasks .","title":"FAQ"},{"location":"dev/faq/#development-faqs","text":"","title":"Development FAQs"},{"location":"dev/faq/#when-do-i-have-to-rebuild-the-docker-image","text":"When you have changed build.sbt or any scala files in reader.src .","title":"When do I have to rebuild the Docker image?"},{"location":"dev/faq/#how-do-i-rebuild-the-docker-image","text":"To rebuild the Docker image, simply run sbt dockerize in your terminal. Make sure you are in the correct directory, which should be where you cloned the cgiar-reader repo.","title":"How do I rebuild the Docker image?"},{"location":"dev/faq/#there-are-two-identical-grammars-in-different-directories-where-do-i-add-rules","text":"Always add/change rules in the yml files under reader.grammars.cgiar . The changes made to these files will be updated in the src files automatically.","title":"There are two identical grammars in different directories, where do I add rules?"},{"location":"dev/faq/#how-do-i-change-annotation-pipelines","text":"The config file passed to the API upon startup should include a specification for the preffered processor. Currently only \"ProxiedProcessor\", \"CluProcessor\", and \"Custom\" are supported processor options. To specify a custom processor use \"ProxiedProcessor.\"","title":"How do I change annotation pipelines?"},{"location":"dev/faq/#what-variables-can-i-specify-in-the-config-file","text":"The user can specify RULES_PREFIX and PROCESSORS_SERVICE_URL . The latter is the url of the proxied processor service.","title":"What variables can I specify in the config file?"},{"location":"dev/faq/#what-sbt-command-line-options-are-there","text":"To see what commands are available, run sbt tasks .","title":"What sbt command line options are there?"},{"location":"dev/howitworks/","text":"How it Works \uf0c1 The /api/extract endpoint will be used as a running example of the cgiar reader workflow. The api endpoints are defined in rest.app.controllers.ApiController.scala . The following components of the cgiar reader can be developed following the Odin manual . Rules \uf0c1 Rules (and the taxonomy) are written in YAML . To develop, rules should be added to the entities and events grammars in reader.grammars.cgiar , NOT reader.src.main.resources.ai.lum.reader.grammars.cgiar . When the api is launched, the reader.grammars.cgiar directory is used to update the congruent directory in reader.src , which is used in defining the MachineReadingSystem over which the api endpoints are run. Rules should be developed in a test oriented manner. That is, tests should be written before the rules. Tests should capture what the expected output of the new rules should be, based on example inputs and the intended purpose. The rules are split into separate entities and events grammars. This is due to the workflow of the MachineReadingSystem which makes entity mentions available to the EventFinder (which runs the events grammar over the entity mentions). Both the entities and events grammars include a master.yml and auxiliary grammars. These auxiliary grammars are imported into master, and thus only master is used in defining in the EntityFinder and EventFinder . Taxonomy \uf0c1 The taxonomy should be developed alongside the rules in the same manner. Always add to the taxonomy under reader.grammars.cgiar and NOT the corresponding directory under reader.src . The taxonomy represents the hiearchical structure of the entities and events being extracted. For example, if a rule produces the label Date , the resultant mention in the json file will be displayed with the labels Constraint, TimeConstraint, TimeExpression, Date , representing the hypernymic relationship of the rule's label. Actions \uf0c1 Actions dictate how text is annotated, how rules are applied to annotated text, and the final structure of the resultant mentions. The MachineReadingSystem stipulates that the EntityFinder and EventFinder have three types of actions: actions, globalAction, and finalAction. The location of these actions is found in the config file used to initialize the MachineReadingSystem. For the running example this is reference.conf , which sets the actions for both the Entity and Event Finder as CustomActions.scala , which can be found under reader.src.main.resources.ai.lum.mr.cgiar.odin . The global and final actions differ for the EntityFinder and EventFinder , as is expected given the flow of extraction. Global actions can be found in OdinActions.scala , under reader.src.main.resources.scala.ai.lum.mr.actions . Under the running config, the EntityFinder's globalAction is identityAction and the EventFinder's globalAction is cleanupEvents . The final actions are cleanupEntities and finalSweep . These globalActions and finalActions are in line with the cgiar Reader workflow, in which the annotated text is first run through the EntityFinder and then the EventFinder, which is run over the \"cleaned up\" entity mentions, followed by a \"clean up\" of the event mentions and a \"final sweep\" of all resultant mentions. API Workflows \uf0c1 When the REST API is launced an instance of a MachineReadingSystem object is created with the values specified in the config.yml file. The MachineReadingSystem in turn creates an instance of a Processor object, a EntityFinder object, and a EventFinder object. The latter are instanced based on the currently defined entities and events grammars. Text which is run through the /api/extract endpoint is passed to the MachineReadingSystem via its extract() method. This method first runs the text through the Processor object creating Annotated Text , which is then passed to the EntityFinder which finds all entity mentions, runs global actions, then runs the final action (in this case a \"clean up\" action). The entity mentions are then passed to the EventFinder which finds all event mentions and performs final \"clean up\" actions. Thus, a final JSON of all mentions is created for the input text. graph LR subgraph API extract(/api/extract) subgraph MachineReadingSystem proc[Processor]; entf[EntityFinder]; evf[EventFinder]; end end mentions{Mentions} Text--extract-->proc; proc--processors.Document-->entf; entf--EntityMentions-->evf; evf-->mentions;","title":"How it Works"},{"location":"dev/howitworks/#how-it-works","text":"The /api/extract endpoint will be used as a running example of the cgiar reader workflow. The api endpoints are defined in rest.app.controllers.ApiController.scala . The following components of the cgiar reader can be developed following the Odin manual .","title":"How it Works"},{"location":"dev/howitworks/#rules","text":"Rules (and the taxonomy) are written in YAML . To develop, rules should be added to the entities and events grammars in reader.grammars.cgiar , NOT reader.src.main.resources.ai.lum.reader.grammars.cgiar . When the api is launched, the reader.grammars.cgiar directory is used to update the congruent directory in reader.src , which is used in defining the MachineReadingSystem over which the api endpoints are run. Rules should be developed in a test oriented manner. That is, tests should be written before the rules. Tests should capture what the expected output of the new rules should be, based on example inputs and the intended purpose. The rules are split into separate entities and events grammars. This is due to the workflow of the MachineReadingSystem which makes entity mentions available to the EventFinder (which runs the events grammar over the entity mentions). Both the entities and events grammars include a master.yml and auxiliary grammars. These auxiliary grammars are imported into master, and thus only master is used in defining in the EntityFinder and EventFinder .","title":"Rules"},{"location":"dev/howitworks/#taxonomy","text":"The taxonomy should be developed alongside the rules in the same manner. Always add to the taxonomy under reader.grammars.cgiar and NOT the corresponding directory under reader.src . The taxonomy represents the hiearchical structure of the entities and events being extracted. For example, if a rule produces the label Date , the resultant mention in the json file will be displayed with the labels Constraint, TimeConstraint, TimeExpression, Date , representing the hypernymic relationship of the rule's label.","title":"Taxonomy"},{"location":"dev/howitworks/#actions","text":"Actions dictate how text is annotated, how rules are applied to annotated text, and the final structure of the resultant mentions. The MachineReadingSystem stipulates that the EntityFinder and EventFinder have three types of actions: actions, globalAction, and finalAction. The location of these actions is found in the config file used to initialize the MachineReadingSystem. For the running example this is reference.conf , which sets the actions for both the Entity and Event Finder as CustomActions.scala , which can be found under reader.src.main.resources.ai.lum.mr.cgiar.odin . The global and final actions differ for the EntityFinder and EventFinder , as is expected given the flow of extraction. Global actions can be found in OdinActions.scala , under reader.src.main.resources.scala.ai.lum.mr.actions . Under the running config, the EntityFinder's globalAction is identityAction and the EventFinder's globalAction is cleanupEvents . The final actions are cleanupEntities and finalSweep . These globalActions and finalActions are in line with the cgiar Reader workflow, in which the annotated text is first run through the EntityFinder and then the EventFinder, which is run over the \"cleaned up\" entity mentions, followed by a \"clean up\" of the event mentions and a \"final sweep\" of all resultant mentions.","title":"Actions"},{"location":"dev/howitworks/#api-workflows","text":"When the REST API is launced an instance of a MachineReadingSystem object is created with the values specified in the config.yml file. The MachineReadingSystem in turn creates an instance of a Processor object, a EntityFinder object, and a EventFinder object. The latter are instanced based on the currently defined entities and events grammars. Text which is run through the /api/extract endpoint is passed to the MachineReadingSystem via its extract() method. This method first runs the text through the Processor object creating Annotated Text , which is then passed to the EntityFinder which finds all entity mentions, runs global actions, then runs the final action (in this case a \"clean up\" action). The entity mentions are then passed to the EventFinder which finds all event mentions and performs final \"clean up\" actions. Thus, a final JSON of all mentions is created for the input text. graph LR subgraph API extract(/api/extract) subgraph MachineReadingSystem proc[Processor]; entf[EntityFinder]; evf[EventFinder]; end end mentions{Mentions} Text--extract-->proc; proc--processors.Document-->entf; entf--EntityMentions-->evf; evf-->mentions;","title":"API Workflows"},{"location":"dev/install/","text":"Installing cgiar-reader (For Development) \uf0c1 Requirements \uf0c1 docker JDK 11 sbt 8G of RAM Install \uf0c1 Clone the cgiar-reader repository. Note We suggest developing the cgiar-reader on a Linux environment under the ~/repos/cgiar-reader directory. This documentation contains commands which run under these assumptions. To run the visualizer alongside the cgiar-reader REST API, clone the odin-tuorial repository and change the docker-compose.yml file to: version: \"3.8\" services: frontend: image: parsertongue/odin-tutorial:local build: context: . dockerfile: Dockerfile restart: unless-stopped ports: - \"8880:7777\" env_file: - ./.env environment: ODIN_API_BASE_URL: http://0.0.0.0:9000/api Running \uf0c1 To run the cgiar-reader REST API in development mode, run the following command under the cgiar-reader directory to redirect to the external grammars: RULES_PREFIX=file://$HOME/repos/cgiar-reader/cgiar-reader/reader/grammars/cgiar sbt web Open your browser to localhost:9000 . Note If you didn't clone the cgiar-reader repository under the recomended directory, change the RULES_PREFIX path to reflect your local path. To run the visualizer without docker , you'll need a version of Node installed. Clone the odin-tutorial repo run the command from the project root: npm install && npm run start Open your browser to localhost:7777/playground .","title":"Install"},{"location":"dev/install/#installing-cgiar-reader-for-development","text":"","title":"Installing cgiar-reader (For Development)"},{"location":"dev/install/#requirements","text":"docker JDK 11 sbt 8G of RAM","title":"Requirements"},{"location":"dev/install/#install","text":"Clone the cgiar-reader repository. Note We suggest developing the cgiar-reader on a Linux environment under the ~/repos/cgiar-reader directory. This documentation contains commands which run under these assumptions. To run the visualizer alongside the cgiar-reader REST API, clone the odin-tuorial repository and change the docker-compose.yml file to: version: \"3.8\" services: frontend: image: parsertongue/odin-tutorial:local build: context: . dockerfile: Dockerfile restart: unless-stopped ports: - \"8880:7777\" env_file: - ./.env environment: ODIN_API_BASE_URL: http://0.0.0.0:9000/api","title":"Install"},{"location":"dev/install/#running","text":"To run the cgiar-reader REST API in development mode, run the following command under the cgiar-reader directory to redirect to the external grammars: RULES_PREFIX=file://$HOME/repos/cgiar-reader/cgiar-reader/reader/grammars/cgiar sbt web Open your browser to localhost:9000 . Note If you didn't clone the cgiar-reader repository under the recomended directory, change the RULES_PREFIX path to reflect your local path. To run the visualizer without docker , you'll need a version of Node installed. Clone the odin-tutorial repo run the command from the project root: npm install && npm run start Open your browser to localhost:7777/playground .","title":"Running"},{"location":"dev/test/","text":"Testing \uf0c1 Tests should be developed before rules. When developing rules, tests can be run using the command: sbt cleanTest Note This command first copies the grammars to the src files before running the tests. To run the tests without updating the rules use the command sbt test . Developing Tests \uf0c1 Testing is performed through two files: TestEntities.scala (reader/src/test/scala/ai/lum/mr/cgiar/entities/TestEntities.scala) and TestEvents.scala (reader/src/test/scala/ai/lum/mr/cgiar/events/TestEvents.scala), that utilize the testing functions defined in TestUtils.scala (reader/src/test/scala/ai/lum/mr/TestUtils.scala). Both TestEntities.scala and TestEvents.scala call the same boolean function, checkMention() . checkMention operates on two inputs: a sequence of Mentions (extracted by the MachineReadingSystem using the Odin rules in cgiar-reader), and a MentionTestCase . MentionTestCase is split into two cases: ExistsMentionTestCase , and ForAllMentionTestCase . The names of the two cases indicate the quantifier applying to evaluation of the sequence of Mentions, i.e. find at least one Mention such that..., or insure that for all Mentions ... . This different quantification is directly reflected in the evaluation procedure at the level of checkMention : ExistsMentionTestCase : // At least one mention must meet the criteria mentions.exists { m => em.check(m) } ForAllMentionTestCase : // Every mention must meet the criteria mentions.forall { m => em.check(m) } All functions called by checkMention have their own check() boolean function. MentionTestCase (either variety)'s check function calls the check functions of its TextTestCase and LabelTestCase(s) obligatorily, and calls check from ArgTestCase if present. ArgTestCase , in turn, through its check function, evaluates the check functions of its own TextTestCase , LabelTestCase (s), and RoleTestCase (s). The bottom-level functions TextTestCase , LabelTestCase , RoleTestCase each have two varieties indicated by prefix: Positive- / Negative- . As the names suggest, a PositiveTextTestCase returns true if its string argument matches the text value of the Mention submitted to its check function. A NegativeTextTestCase returns true if its string argument does NOT match the Mention's text field. Entity and Event tests differ in that Entity tests cannot contain ArgTestCase or RoleTestCase , only TextTestCase and LabelTestCase checks. The typical use of testing involves positive subtests embedded in an ExistsMentionTestCase , as in this example: ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Vessel\")), mentionSpan = PositiveTextTestCase(\"LNG Finima\"), text = \"LNG Finima\" ) To illustrate the potential utility of negative testing: the entity rules include patterns for dates, such as \"12 Jun 2000\". However, a possible confusion arises from the format of time-of-day expressions in military documents, where we might see \"12 Jun 2000 hours.\" (i.e., 8 P.M. on June 12). Of course, the entity rules themselves have a variety of means available to enforce this restriction: negative lookahead in the date rule, higher-priority military-time rules, etc. But to test proper functioning, we can write tests that might either seek to verify that at least one Mention found in some piece of text has such-and-such properties, or deny that any Mentions have some undesirable configuration of properties. Complexities arise because MentionTestCase and ArgTestsCase contain multiple subtests ( TextTestCase , LabelTestCase , RoleTestCase ). What combinations of polarities among subtests (e.g. all positive, all negative, or a mixture of positive and negative) fall within the expected range of use? Consider first the simplest case, corresponding to Entity tests: MentionTestCase not containing any ArgTestCase . Use for a mix of positive and negative tests: ( PositiveLabelTestCase , NegativetextTestCase ),- make sure the given label is picked up but doesn't correspond to this string; ( NegativeLabelTestCase , PositiveTextTestCase )<--make sure this string is not read as... Running with the above example, suppose the parent MentionTestCase has text \"12 Jun 2000 hours\". The MentionTestCase can either be ExistsMentionTestCase , or ForAllMentionTestCase . Combinations of positive and negative subtests under ForAllMentionTestCase seem of little use. For this reason, and to avoid possible confusion, we have written into the function definitions the requirement that ForAllMentionTestCase scopes over all-negative immediate subtests. Below is a typical example of the proper use of all-negative subtests for ForAllMentionTestCase : ForAllMentionTestCase( labels = Seq(NegativeLabelTestCase(\"WhatQuery\")), mentionSpan = NegativeTextTestCase(\"of ostrich feathers\"), text = \"TEUs of ostrich feathers\" ) Combinations of positive and negative subtests under ExistsMentionTestCase are potentially useful. in keeping with the flexibility outlined for some ArgTestCase s below, we allow ExistsMentionTestCase to freely embed either polarity of LabelTestCase and TextTestCase . Example: ExistsMentionTestCase( labels = Seq(NegativeLabelTestCase(\"Date\")), // wrong label mentionSpan = PositiveTextTestCase(\"LNG Finima\"), text = \"LNG Finima\" ) ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Vessel\")), mentionSpan = NegativeTextTestCase(\"Finima\"), // mentionSpan doesn't match text text = \"LNG Finima\" ) ArgTestCase polarity adds a further layer of potential complexity. To simplify use, we restrict the polarity of ArgTestCase itself to be strictly Negative under ForAllMentionTestCase , just like all its immediate subtests.: ForAllMentionTestCase ( NegativeLabelTestCase , NegativeTextTestCase , NegativeArgTestCase (...)). In this configuration, we restrict NegativeArgTestCase to embed strictly positive subtests: PositiveTextTestCase , PositiveLabelTestCase , and PositiveRoleTestCase . Allowing NegativeArgTestCase to embed negative subtests, or a mix of positive and negative subtests, adds unwanted complexity unrewarded by clear use cases. The example below illustrates the intended use: ForAllMentionTestCase( labels = Seq(NegativeLabelTestCase(\"CargoQuery\"), NegativeLabelTestCase(\"QuantityQuery\")), text = \"Some zebras are galloping to Scotland from Zimbabwe\", mentionSpan = NegativeTextTestCase(\"Some zebras are heading to Scotland from Zimbabwe\"), args = List( NegativeArgTestCase( role = PositiveRoleTestCase(\"need\"), labels = Seq(PositiveLabelTestCase(\"QuantifiedCargo\")), text = PositiveTextTestCase(\"zebras\") ) ) ) ExistsMentionTestCase may embed PositiveArgTestCase and/or NegativeArgTestCase : ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( NegativeArgTestCase( role = PositiveRoleTestCase(\"need\"), labels = Seq(PositiveLabelTestCase(\"BeforeTime\"), PositiveLabelTestCase(\"TimeExpression\")), text = PositiveTextTestCase(\"before September 21st 2020\") ), PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(PositiveLabelTestCase(\"AfterTime\"), PositiveLabelTestCase(\"TimeExpression\")), text = PositiveTextTestCase(\"after September 28th 2020\") ) ), ), To allow desired flexibility with easily-understood uses, we allow the subtests within PositiveArgTestCase (only) to be positive or negative, independently of other subtests within the same PositiveArgTestCase . The example below illustrates: // Negative Label test; positive Text, Role tests. ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(NegativeLabelTestCase(\"AfterTime\")), text = PositiveTextTestCase(\"before September 21st 2020\") ), ) ), // Negative Text test; positive Label, Role tests. ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(PositiveLabelTestCase(\"BeforeTime\")), text = NegativeTextTestCase(\"after September 21st 2020\") ), ) )","title":"Testing"},{"location":"dev/test/#testing","text":"Tests should be developed before rules. When developing rules, tests can be run using the command: sbt cleanTest Note This command first copies the grammars to the src files before running the tests. To run the tests without updating the rules use the command sbt test .","title":"Testing"},{"location":"dev/test/#developing-tests","text":"Testing is performed through two files: TestEntities.scala (reader/src/test/scala/ai/lum/mr/cgiar/entities/TestEntities.scala) and TestEvents.scala (reader/src/test/scala/ai/lum/mr/cgiar/events/TestEvents.scala), that utilize the testing functions defined in TestUtils.scala (reader/src/test/scala/ai/lum/mr/TestUtils.scala). Both TestEntities.scala and TestEvents.scala call the same boolean function, checkMention() . checkMention operates on two inputs: a sequence of Mentions (extracted by the MachineReadingSystem using the Odin rules in cgiar-reader), and a MentionTestCase . MentionTestCase is split into two cases: ExistsMentionTestCase , and ForAllMentionTestCase . The names of the two cases indicate the quantifier applying to evaluation of the sequence of Mentions, i.e. find at least one Mention such that..., or insure that for all Mentions ... . This different quantification is directly reflected in the evaluation procedure at the level of checkMention : ExistsMentionTestCase : // At least one mention must meet the criteria mentions.exists { m => em.check(m) } ForAllMentionTestCase : // Every mention must meet the criteria mentions.forall { m => em.check(m) } All functions called by checkMention have their own check() boolean function. MentionTestCase (either variety)'s check function calls the check functions of its TextTestCase and LabelTestCase(s) obligatorily, and calls check from ArgTestCase if present. ArgTestCase , in turn, through its check function, evaluates the check functions of its own TextTestCase , LabelTestCase (s), and RoleTestCase (s). The bottom-level functions TextTestCase , LabelTestCase , RoleTestCase each have two varieties indicated by prefix: Positive- / Negative- . As the names suggest, a PositiveTextTestCase returns true if its string argument matches the text value of the Mention submitted to its check function. A NegativeTextTestCase returns true if its string argument does NOT match the Mention's text field. Entity and Event tests differ in that Entity tests cannot contain ArgTestCase or RoleTestCase , only TextTestCase and LabelTestCase checks. The typical use of testing involves positive subtests embedded in an ExistsMentionTestCase , as in this example: ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Vessel\")), mentionSpan = PositiveTextTestCase(\"LNG Finima\"), text = \"LNG Finima\" ) To illustrate the potential utility of negative testing: the entity rules include patterns for dates, such as \"12 Jun 2000\". However, a possible confusion arises from the format of time-of-day expressions in military documents, where we might see \"12 Jun 2000 hours.\" (i.e., 8 P.M. on June 12). Of course, the entity rules themselves have a variety of means available to enforce this restriction: negative lookahead in the date rule, higher-priority military-time rules, etc. But to test proper functioning, we can write tests that might either seek to verify that at least one Mention found in some piece of text has such-and-such properties, or deny that any Mentions have some undesirable configuration of properties. Complexities arise because MentionTestCase and ArgTestsCase contain multiple subtests ( TextTestCase , LabelTestCase , RoleTestCase ). What combinations of polarities among subtests (e.g. all positive, all negative, or a mixture of positive and negative) fall within the expected range of use? Consider first the simplest case, corresponding to Entity tests: MentionTestCase not containing any ArgTestCase . Use for a mix of positive and negative tests: ( PositiveLabelTestCase , NegativetextTestCase ),- make sure the given label is picked up but doesn't correspond to this string; ( NegativeLabelTestCase , PositiveTextTestCase )<--make sure this string is not read as... Running with the above example, suppose the parent MentionTestCase has text \"12 Jun 2000 hours\". The MentionTestCase can either be ExistsMentionTestCase , or ForAllMentionTestCase . Combinations of positive and negative subtests under ForAllMentionTestCase seem of little use. For this reason, and to avoid possible confusion, we have written into the function definitions the requirement that ForAllMentionTestCase scopes over all-negative immediate subtests. Below is a typical example of the proper use of all-negative subtests for ForAllMentionTestCase : ForAllMentionTestCase( labels = Seq(NegativeLabelTestCase(\"WhatQuery\")), mentionSpan = NegativeTextTestCase(\"of ostrich feathers\"), text = \"TEUs of ostrich feathers\" ) Combinations of positive and negative subtests under ExistsMentionTestCase are potentially useful. in keeping with the flexibility outlined for some ArgTestCase s below, we allow ExistsMentionTestCase to freely embed either polarity of LabelTestCase and TextTestCase . Example: ExistsMentionTestCase( labels = Seq(NegativeLabelTestCase(\"Date\")), // wrong label mentionSpan = PositiveTextTestCase(\"LNG Finima\"), text = \"LNG Finima\" ) ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Vessel\")), mentionSpan = NegativeTextTestCase(\"Finima\"), // mentionSpan doesn't match text text = \"LNG Finima\" ) ArgTestCase polarity adds a further layer of potential complexity. To simplify use, we restrict the polarity of ArgTestCase itself to be strictly Negative under ForAllMentionTestCase , just like all its immediate subtests.: ForAllMentionTestCase ( NegativeLabelTestCase , NegativeTextTestCase , NegativeArgTestCase (...)). In this configuration, we restrict NegativeArgTestCase to embed strictly positive subtests: PositiveTextTestCase , PositiveLabelTestCase , and PositiveRoleTestCase . Allowing NegativeArgTestCase to embed negative subtests, or a mix of positive and negative subtests, adds unwanted complexity unrewarded by clear use cases. The example below illustrates the intended use: ForAllMentionTestCase( labels = Seq(NegativeLabelTestCase(\"CargoQuery\"), NegativeLabelTestCase(\"QuantityQuery\")), text = \"Some zebras are galloping to Scotland from Zimbabwe\", mentionSpan = NegativeTextTestCase(\"Some zebras are heading to Scotland from Zimbabwe\"), args = List( NegativeArgTestCase( role = PositiveRoleTestCase(\"need\"), labels = Seq(PositiveLabelTestCase(\"QuantifiedCargo\")), text = PositiveTextTestCase(\"zebras\") ) ) ) ExistsMentionTestCase may embed PositiveArgTestCase and/or NegativeArgTestCase : ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( NegativeArgTestCase( role = PositiveRoleTestCase(\"need\"), labels = Seq(PositiveLabelTestCase(\"BeforeTime\"), PositiveLabelTestCase(\"TimeExpression\")), text = PositiveTextTestCase(\"before September 21st 2020\") ), PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(PositiveLabelTestCase(\"AfterTime\"), PositiveLabelTestCase(\"TimeExpression\")), text = PositiveTextTestCase(\"after September 28th 2020\") ) ), ), To allow desired flexibility with easily-understood uses, we allow the subtests within PositiveArgTestCase (only) to be positive or negative, independently of other subtests within the same PositiveArgTestCase . The example below illustrates: // Negative Label test; positive Text, Role tests. ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(NegativeLabelTestCase(\"AfterTime\")), text = PositiveTextTestCase(\"before September 21st 2020\") ), ) ), // Negative Text test; positive Label, Role tests. ExistsMentionTestCase( labels = Seq(PositiveLabelTestCase(\"Transport\")), mentionSpan = PositiveTextTestCase(\"Frozen food that arrived before September 21st 2020 but after September 28th 2020\"), text = \"Frozen food that arrived before September 21st 2020 but after September 28th 2020.\", args = List( PositiveArgTestCase( role = PositiveRoleTestCase(\"time\"), labels = Seq(PositiveLabelTestCase(\"BeforeTime\")), text = NegativeTextTestCase(\"after September 21st 2020\") ), ) )","title":"Developing Tests"}]}